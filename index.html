<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Accelerating SqueezeNet on FPGA</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<span class="logo"><img src="images/logo.svg" alt="" /></span>
						<h1>Accelerating SqueezeNet on FPGA</h1>
						<p>by <a href="http://in.linkedin.com/in/meghaarora42">Megha Arora</a> and <a href="https://">Samyukta Lanka</a>.</p>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#final">Final</a></li>
							<li><a href="#proposal" class="active">Initial Proposal</a></li>
							<!-- <li><a href="#first">CheckPoint</a></li> -->
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
							<!-- Second Section -->
							<section id="final" class="main">
								<!-- <div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Final Title</h2>
										</header>
										<p> Accelerating SqueezeNet on Zybo Zynq-7020 FPGA</p>
									</div>
								</div> -->
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Project Summary</h2>
										</header>
										<p> We have successfully been able to accelerate the <a href="https://arxiv.org/pdf/1602.07360.pdf">SqueezeNet</a> on Zybo Zynq-7020 FPGA. Our implementation (when evaluated using the ILSVRC2012 ImageNet data) is faster and more energy efficient as compared to our <a href="#baseline">baseline</a>! </p>
									</div>
								</div>

								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Background</h2>
										</header>

											SqueezeNet is an 18-layer network that uses 1x1 and 3x3 convolutions, 3x3 max-pooling and global-averaging. One of its major components is the <b>fire layer</b>. Fire layers start out with a "squeeze" step (a few 1x1 convolutions) and lead to two "expand" steps, which include a 1x1 and a 3x3 convolution followed by concatenation of the two results. SqueezeNet was created to combat the large number of parameters required for CNNs. The design goal was to reduce the total number of weights without losing accuracy significantly. SqueezeNet v1.1 has 1.24 million parameters i.e. around <b>5MB</b> of weights in all. <br>
											One of our goals was to parallelize the convolutions, which are the primary components of our workload. We also needed to ensure that we don't parallelize at the expense of performing more work than the sequential approach, access cache poorly and use inefficient storage patterns. <br>
											There are various restrictions that come with the workload and the FPGA setting. For instance, we are bound to be sequential in between layers. Moreover, we cannot perform the expand step before the squeeze step finishes. Essentially, there are synchronization requirements within the workload, which we cannot overlook. We mention some of the major technical challenges we faced in the next section.
									</div>
									<img src="images/Fire.png" alt="" height="185" width="300" /> <br>
								</div>

								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Technical Challenges</h2>
										</header>
										<li>Being new to development on FPGAs meant there was a steep learning curve. Even when our simulations synthesized on Vivado, we faced a lot of trouble running them directly on the FPGA</li>
										<li>Understanding memory hierarchy of Zybo Zynq-7020 FPGA and optimizing data movement across SD Card, DRAM, BRAM and the registers</li>
										<li>DNN workload mostly hits the memory wall</li>
										<li>Multiple levels of parallelism in the DNN workload & associated synchronization requirements</li>
									</div>
								</div>

								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Approach and Optimizations</h2>
										</header>
										<p> We obtained the trained weights from the original <a href="https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1">Caffe model</a> provided by the authors of SqueezeNet. However, we did not have an existing code-base for the FPGA implementation to build upon.<br>

										We store weights and image data on the DRAM in the FPGA. For each layer computation, the weights are loaded into the BRAM along with the input data for that layer. There are several ways we could do this! We optimized our access and storage patterns for <b>maximal data reuse</b>. We also ensured that our approach incorporates <b>producer-consumer locality</b> by fusing multiple layers together. As a first attempt (we call it <b>Layer Fusion 1</b>), instead of loading the squeeze and the two expand layers separately on the FPGA, we load the layer settings at once. Essentially, this brings down the number of layers in our network from 18 to 10. We perform computations on a batch of images at a time to amortize over loading layer settings and weights time.<br><br>
										<img src="images/lf1.png" alt="" height="350" width="800"/> <br><br>

										We tried to improve performance of our convolutions via pipelining, loop unrolling and flattening, but it didn't help because we are bandwidth bound, so making our compute faster won't have an impact. <br>

										Another optimization (we call it <b>Layer Fusion 2</b>) that we attempted was fusing convolution and max-pool layers. Originally, we performed a convolution (and ReLU), the output was written back, and then loaded again to perform a max-pool. By fusing these layers, we were able to max-pool right after performing the convolution on a smaller chunks, thus improving producer-consumer locality.</p> <br>
										<img src="images/lf2.png" alt="" height="250" width="750" style="text-align:center"/> <br>
									</div>

								</div>


								<div id="baseline" class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Baseline</h2>
										</header>
										<li> Evaluation using the SqueezeNet v1.1 Keras model on GeForce GTX 1080 GPU on GHC machines </li>
										<li> Weights obtained from original <a href="https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1">Caffe model</a> by authors of SqueezeNet </li> 
										<li> #images - 1000 images (224x224) from <a href="http://www.image-net.org/challenges/LSVRC/2012/"> ILSVRC2012 ImageNet data </a></li>
									</div>
								</div>


								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Initial Results</h2>
										</header>

										<img src="images/Conv.png" alt="" /> <br>

										<p>These are estimates generated using the HLS Synthesis tool specific to SqueezeNetv1.1. This shows the ideal decrease in the total number of clock cycles required for the entire SqueezeNet workload. After pipelining, loop flattening and unrolling, we only need 2% of the total clock cycles we need otherwise (without any of the optimizations)! Seems like an efficient implementation, right? We don't see this improvement once we put the SqueezeNet on the FPGA. :(</p>	

										<img src="images/Initial.png" alt="" /> <br>
										<p>We get a speedup of around <b>4.31x</b> as compared to the GPU baseline. The implementation represented by the red bar includes Layer Fusion 1 (amortizing over loading layers), maximizing consumer-producer locality, input reuse and making access patterns efficient! As is evident, any of the optimizations for convolutions i.e. pipelining and loop unrolling (represented by the yellow bar) did not help us run any faster as we are memory bandwidth bound on the FPGA!</p>								
									</div>
								</div>

								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Final Results</h2>
										</header>
										<img src="images/Final.png" alt="" /> <br>
										<p>We also tried several HLS enables optimization by using directives like ARRAY_PARTITION, RESOURCE, FUNCTION_INSTANTIATE, etc. After Layer Fusion 2 and these HLS enabled optimizations, we get a net speedup of around <b>4.96x</b> as compared to the GPU baseline. Also, note that our <b>accuracy</b> is exactly the same as the output from our <a href="baseline"> baseline </a> for the 1000 images as we use the exact same weights and perform the same computation (just more efficiently)!</p>

										<header>
											<h3><b>Final Resource Utilization</b></h3>
										</header>
										<img src="images/Resource.png" alt="" height="200" width="800" /> <br>
										<p>These estimates are generated using the HLS Synthesis tool specifically for SqueezeNet v1.1. Our BRAM utilization is high! We are also using compute resources well. We mainly care about BRAM here due to the nature of our workload.</p>

										<header>
											<h3><b>Energy Efficiency</b></h3>
										</header>
										<img src="images/Power.png" alt="" height="260" width="600"/> <br>
										<p>These are the power measurements for the SqueezeNet FPGA Accelerator when synthesized for Zybo Zynq-7020 FPGA as compared to the GPU baseline. FPGA estimations have been obtained using the <a href="https://www.xilinx.com/products/technology/power/xpe.html">Xilinx Power Estimator (XCE) tool</a> and the GPU measurements using the nvidia-smi interface. Our implementation is significantly more energy efficient which is inline with what we have learnt about hardware specialization from this class!</p>


									</div>
								</div>


								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Potential Improvement - Fixed Point Arithmetic!</h2>
										</header>
										<p>According to a quick test synthesis with Vivado HLS, there is a potential to save 50% of the BRAMs and 80% of the DSP slices if we use a 16-bit fixed-point data format as compared to the current 32-bit representation.</p>
									</div>
								</div> 

								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>References</h2>
										</header>
										<li> [0]: <a href="https://arxiv.org/pdf/1602.07360.pdf"> https://arxiv.org/pdf/1602.07360.pdf</a></li>
										<li> [1]: <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"> https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf </a></li>
										<li> [2]: <a href="https://github.com/dgschwend/zynqnet"> https://github.com/dgschwend/zynqnet</a></li>
										<li> [3]: <a href="http://lab.fs.uni-lj.si/lasin/wp/IMIT_files/neural/doc/Omondi2006.pdf"> http://lab.fs.uni-lj.si/lasin/wp/IMIT_files/neural/doc/Omondi2006.pdf</a></li>										
									</div>
								</div>

								<!-- <ul class="statistics">
									<li class="style1">
										<span class="icon fa-code-fork"></span>
										<strong>5,120</strong> Etiam
									</li>
									<li class="style2">
										<span class="icon fa-folder-open-o"></span>
										<strong>8,192</strong> Magna
									</li>
									<li class="style3">
										<span class="icon fa-signal"></span>
										<strong>2,048</strong> Tempus
									</li>
									<li class="style4">
										<span class="icon fa-laptop"></span>
										<strong>4,096</strong> Aliquam
									</li>
									<li class="style5">
										<span class="icon fa-diamond"></span>
										<strong>1,024</strong> Nullam
									</li>
								</ul>
								<p class="content">Nam elementum nisl et mi a commodo porttitor. Morbi sit amet nisl eu arcu faucibus hendrerit vel a risus. Nam a orci mi, elementum ac arcu sit amet, fermentum pellentesque et purus. Integer maximus varius lorem, sed convallis diam accumsan sed. Etiam porttitor placerat sapien, sed eleifend a enim pulvinar faucibus semper quis ut arcu. Ut non nisl a mollis est efficitur vestibulum. Integer eget purus nec nulla mattis et accumsan ut magna libero. Morbi auctor iaculis porttitor. Sed ut magna ac risus et hendrerit scelerisque. Praesent eleifend lacus in lectus aliquam porta. Cras eu ornare dui curabitur lacinia.</p>
								<footer class="major">
									<ul class="actions">
										<li><a href="generic.html" class="button">Learn More</a></li>
									</ul>
								</footer> -->
							</section>


						<!-- Introduction -->
							<section id="proposal" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Project Goal</h2>
										</header>
										<p>We will accelerate <a href="https://arxiv.org/pdf/1602.07360.pdf">SqueezeNet</a> (a CNN variant) on an FPGA</p>
									</div>
								</div>
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Background</h2>
										</header>

											<p>We chose DNNs because they are revolutionizing many areas of computing today and there is parallelism that the underlying model possesses. The sequence of multiply/add operations (like FMA) to calculate the input to each layer in the network are highly parallelizable. In particular, there are multiple axis of parallelism we plan to explore - </p>

											<li><b>Layer parallelism</b> - different layers being processed in parallel (low level of parallelism). [maybe use pipelining to exploit this] </li>

											<li><b>Node parallelism</b> - Here the axis of parallelism will be each neuron (there can be millions in a DNN); most prominent in this problem. Entails high level of parallelism and FPGA ‘cells’ are ideal for this. </li>

											<li><b>Data parallelism</b> - Computing the dot product of the input and the weights at each step (including convolution) in parallel. </li>

											<p>An extremely popular DNN is Covolutional Neural Network(CNN) which is extensively used in the domain of computer vision. SqueezeNet was created to combat the large number of parameters required for CNNs. Based on <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a> , the purpose was to reduce the memory required without losing accuracy. We will be accelerating SqueezeNet (architecture on right) on the Zybo Zynq-7020 FPGA.</p>
									</div>
									<img src="images/SqueezeNet.PNG" alt="" /> <br>
								</div>
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Challenges</h2>
										</header>
										<p>All the different levels of parallelism vary drastically and will be traded off against each other. Some are more prominent in the problem as compared to others. Varying amount of effort and benefit is involved in each, which would require extensive amount of platform knowledge (FPGA in this case) and testing multiple implementations! Deciding which one to prioritize at what cost will be challenging! Also, some types of parallelism may not be suitable for us to implement on the FPGA depending on the design (eg: exploiting bit-level parallelism on FPGA). </p>

										<p> One of the major challenges that we will be dealing with is being memory bound on the FPGA. The specific device that we have has 256 KB of on-chip memory with options for external memory. The size of SqueezeNet is much larger than this and it would be difficult to fit the model on the chip. This means we will also have to ensure data locality and reuse.</p>
									</div>
								</div>
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Resources</h2>
										</header>
										<p>We will use the Zybo Zynq-7020 FPGA for this task. We do not have a codebase to refer to; the initial FPGA implementation will be our first goal. We will be referring to the following implementations/papers extensively - </p>

										<li><a href="https://arxiv.org/pdf/1602.07360.pdf">SqueezeNet</a></li>
										<li><a href="https://github.com/dgschwend/zynqnet">Implementation of SqueezeNet-like CNN on FPGA</a></li>
										<li><a href="http://lab.fs.uni-lj.si/lasin/wp/IMIT_files/neural/doc/Omondi2006.pdf">FPGA Implementations of Neural Networks</a></li>
									</div>
								</div>
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Goals and Deliverables</h2>
										</header>
										<li>Plan to Achieve: Get a <b>working baseline FPGA implementation that exceeds CPU performance without compromising accuracy</b></li>

										<li>Hope to Achieve: Fit the model in on-chip memory </li>

										<li>Demo: Simultaneously show SqueezeNet in action on multiple images with one version running the CPU implementation and the other running the FPGA implementation. We will also present the performance of our implementation as compared to <a href="https://github.com/soumith/convnet-benchmarks">existing benchmarks</a> through speedup graphs. </li>
									</div>
								</div>
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Platform</h2>
										</header>
										<p> General purpose processors fail to fully exploit the parallelism inherent in DNNs. For instance it can take upto 3 weeks to train a 22 layer CNN for image classification (ImageNet) using 1 NVIDIA K20 GPU. The drawback of using an ASIC neurocomputer for this task also has its own challenges. First, devices that work on special applications always lag behind the mass-produced devices in terms of innovation. Given the variability in ANN architectures, the ASIC neurocomputer cannot be extended to many different architectures without involving significant cost! </p>

										<p>FPGAs on the other hand offer the best balance of cost:performance and flexibility for this domain as compared to ASIC neurocomputers or state-of-the-art multi-purpose processors. Essentially, they can be used as <b>‘semi-custom’ machines for neural networks.</b></p>
									</div>
								</div>

								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Schedule</h2>
										</header>
										<table style="width:100%">
										  <tr>
										    <th>Date</th>
										    <th>Milestone</th> 
										  </tr>
										  <tr>
										    <td>April 10</td>
										    <td>Proposal</td> 
										  </tr>
										  <tr>
										    <td>April 16</td>
										    <td>Convolutions and Max Pooling Layers Working Separately on FPGA</td> 
										  </tr>
										  <tr>
										    <td>April 23</td>
										    <td>SqueezeNet Working on FPGA</td> 
										  </tr>
										  <tr>
										    <td>April 30</td>
										    <td>Acceleration + Optimizations</td> 
										  </tr>
										  <tr>
										    <td>May 7</td>
										    <td>Benchmarking</td> 
										  </tr>
										  <tr>
										    <td>May 11</td>
										    <td>Result Collation + Report Writing</td> 
										  </tr>
										</table>
									</div>
								</div>

								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>References</h2>
										</header>
										<li> [0]: <a href="https://arxiv.org/pdf/1602.07360.pdf"> https://arxiv.org/pdf/1602.07360.pdf</a></li>
										<li> [1]: <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"> https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf </a></li>
										<li> [2]: <a href="https://github.com/dgschwend/zynqnet"> https://github.com/dgschwend/zynqnet</a></li>
										<li> [3]: <a href="http://lab.fs.uni-lj.si/lasin/wp/IMIT_files/neural/doc/Omondi2006.pdf"> http://lab.fs.uni-lj.si/lasin/wp/IMIT_files/neural/doc/Omondi2006.pdf</a></li>										
									</div>
								</div>
							</section>

						<!-- First Section -->
							<section id="first" class="main">
								<!-- <header class="major">
									<h2>Checkpoint</h2>
								</header> -->
								<!-- <div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Schedule (Updated)</h2>
										</header>
										<table style="width:100%">
										  <tr>
										    <th>Date</th>
										    <th>Milestone</th>
										    <th>Status</th>
										  </tr>
										  <tr>
										    <td>April 10</td>
										    <td>Proposal</td> 
										    <td>Done</td> 
										  </tr>
										  <tr>
										    <td>April 16</td>
										    <td>SqueezeNet CPU Benchmark</td>
										    <td>Done</td>  
										  </tr>
										  <tr>
										    <td>April 20</td>
										    <td>Convolutions and Max Pooling Modules</td>
										    <td>Done</td>  
										  </tr>
										  <tr>
										    <td>April 25</td>
										    <td>Design Flow with DMA</td> 
										    <td>Ongoing</td> 
										  </tr>
										  <tr>
										    <td>April 29</td>
										    <td>SqueezeNet Working on FPGA</td> 
										    <td>-</td> 
										  </tr>
										  <tr>
										    <td>May 3</td>
										    <td>Acceleration + Optimizations</td> 
										    <td>-</td> 
										  </tr>
										  <tr>
										    <td>May 7</td>
										    <td>Benchmarking</td>
										    <td>-</td>  
										  </tr>
										  <tr>
										    <td>May 11</td>
										    <td>Result Collation + Report Writing</td>
										    <td>-</td>  
										  </tr>
										</table>
									</div>
								</div> -->
								<!-- <div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Status Update</h2>
										</header>
										<li>We are a little behind because of the delay in getting access to the FPGA and the initial hardware setup (including driver installations) was bumpy. We will have a better idea once we get SqueezeNet working on the FPGA, which we can test against our CPU benchmark for correctness and performance.</li>

										<li>We have the hardware setup complete and verilog modules for convolution and max pooling are also done. We are currently trying to load data from DMA now as doing the same from the SD card is complicated and we couldn't get that to work.</li>

										<li>We had a update meeting with the course staff (Teguh) to finalize our design flow.</li>

										<li>For the parallelism competition, we are going to simultaneously show SqueezeNet in action on multiple images with one version running the CPU implementation and the other running the FPGA implementation. We will also present the performance of our implementation as compared to <a href="https://github.com/soumith/convnet-benchmarks">existing benchmarks</a> through speedup graphs. </li>
									</div>
								</div> -->

								<!-- <ul class="features">
									<li>
										<span class="icon major style1 fa-code"></span>
										<h3>Ipsum consequat</h3>
										<p>Sed lorem amet ipsum dolor et amet nullam consequat a feugiat consequat tempus veroeros sed consequat.</p>
									</li>
									<li>
										<span class="icon major style3 fa-copy"></span>
										<h3>Amed sed feugiat</h3>
										<p>Sed lorem amet ipsum dolor et amet nullam consequat a feugiat consequat tempus veroeros sed consequat.</p>
									</li>
									<li>
										<span class="icon major style5 fa-diamond"></span>
										<h3>Dolor nullam</h3>
										<p>Sed lorem amet ipsum dolor et amet nullam consequat a feugiat consequat tempus veroeros sed consequat.</p>
									</li>
								</ul>
								<footer class="major">
									<ul class="actions">
										<li><a href="generic.html" class="button">Learn More</a></li>
									</ul>
								</footer> -->
							</section>


				<!-- Footer -->
					<footer id="footer">
					<!-- 	<section>
							<h2>Aliquam sed mauris</h2>
							<p>Sed lorem ipsum dolor sit amet et nullam consequat feugiat consequat magna adipiscing tempus etiam dolore veroeros. eget dapibus mauris. Cras aliquet, nisl ut viverra sollicitudin, ligula erat egestas velit, vitae tincidunt odio.</p>
							<ul class="actions">
								<li><a href="generic.html" class="button">Learn More</a></li>
							</ul>
						</section>
						<section>
							<h2>Etiam feugiat</h2>
							<dl class="alt">
								<dt>Address</dt>
								<dd>1234 Somewhere Road &bull; Nashville, TN 00000 &bull; USA</dd>
								<dt>Phone</dt>
								<dd>(000) 000-0000 x 0000</dd>
								<dt>Email</dt>
								<dd><a href="#">information@untitled.tld</a></dd>
							</dl>
							<ul class="icons">
								<li><a href="#" class="icon fa-twitter alt"><span class="label">Twitter</span></a></li>
								<li><a href="#" class="icon fa-facebook alt"><span class="label">Facebook</span></a></li>
								<li><a href="#" class="icon fa-instagram alt"><span class="label">Instagram</span></a></li>
								<li><a href="#" class="icon fa-github alt"><span class="label">GitHub</span></a></li>
								<li><a href="#" class="icon fa-dribbble alt"><span class="label">Dribbble</span></a></li>
							</ul>
						</section>
						<p class="copyright">&copy; Untitled. Design: <a href="https://html5up.net">HTML5 UP</a>.</p> -->
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>